{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from dataset import EEGAudioDataset\n",
    "from model.CLUB import CLUBSample_group\n",
    "from model.CPC import Cross_CPC\n",
    "from model.VQVAE import VQVAEEncoder,VQVAEDecoder,SemanticDecoder\n",
    "from dataset import EEGAudioDataset\n",
    "from torch_dct import dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公共变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "words_path = r'./feat/words'\n",
    "pt = 'sub-06'\n",
    "test_word = 95\n",
    "config_path = r'./config'\n",
    "model_name = 'cmg_noclip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helft (829, 127) (12960,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(words_path,f'{pt}')\n",
    "filename = os.listdir(folder_path)[test_word]\n",
    "word_info = np.load(os.path.join(folder_path,filename),allow_pickle=True)\n",
    "word=word_info.item()['label']\n",
    "eeg=word_info.item()['eeg']\n",
    "audio=word_info.item()['audio']\n",
    "\n",
    "print(word,eeg.shape,audio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(config_path,f'{model_name}.json'),'r') as f:\n",
    "    cfg = json.load(f)\n",
    "    model_cfg = cfg['model_config']\n",
    "    data_cfg = cfg['data_config']\n",
    "\n",
    "# load config \n",
    "seg_size = model_cfg['seg_size']\n",
    "pred_size = model_cfg['pred_size']\n",
    "# batch_size = model_cfg['batch_size']\n",
    "# end_epoch = model_cfg['epochs'] if argu.epoch is None else argu.epoch\n",
    "lr = model_cfg['lr']\n",
    "b1 = model_cfg['b1']\n",
    "b2 = model_cfg['b2']\n",
    "clip_grad = model_cfg['clip_grad']\n",
    "hidden_dim = model_cfg['hidden_dim']\n",
    "d_model = model_cfg['d_model']\n",
    "nhead = model_cfg['nhead']\n",
    "n_layer = model_cfg['n_layer']\n",
    "n_embedding = model_cfg['n_embedding']\n",
    "mi_iter = model_cfg['mi_iter']\n",
    "\n",
    "data_path = data_cfg['data_path']\n",
    "win_len = data_cfg['win_len']\n",
    "frame_shift = data_cfg['frame_shift']\n",
    "eeg_sr = data_cfg['eeg_sr']\n",
    "audio_sr = data_cfg['audio_sr']\n",
    "pad_mode = data_cfg['pad_mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取高频eeg信号和音频信号的梅尔频谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 127) (162, 40)\n"
     ]
    }
   ],
   "source": [
    "eeg = EEGAudioDataset.extractHG(eeg,eeg_sr,windowLength=win_len,frameshift=frame_shift)\n",
    "melspec = EEGAudioDataset.extractMelSpecs(audio,audio_sr,windowLength=win_len,frameshift=frame_shift)\n",
    "# print(eeg.shape,melspec.shape)\n",
    "if melspec.shape[0]!=eeg.shape[0]:\n",
    "    minlen = min(melspec.shape[0],eeg.shape[0])\n",
    "    melspec = melspec[:minlen,:]\n",
    "    eeg = eeg[:minlen,:]\n",
    "print(eeg.shape,melspec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 16, 127) (162, 16, 40)\n"
     ]
    }
   ],
   "source": [
    "eeg_mean = np.mean(eeg)\n",
    "eeg_std = np.std(eeg)\n",
    "eeg = (eeg-eeg_mean)/eeg_std\n",
    "eeg_list = []\n",
    "mel_list = []\n",
    "hop_size = 1\n",
    "pad_width = ((int(np.floor((seg_size-hop_size)/2.0)),int(np.ceil((seg_size-hop_size)/2.0))),(0,0))\n",
    "pad_eeg = np.pad(eeg,pad_width,mode=pad_mode)\n",
    "pad_mel = np.pad(melspec,pad_width,mode=pad_mode)\n",
    "num_win = int(len(eeg)/float(hop_size))\n",
    "for i in range(num_win):\n",
    "    start = i*hop_size\n",
    "    end = start + seg_size\n",
    "    eeg_list.append(pad_eeg[start:end,:])\n",
    "    mel_list.append(pad_mel[start:end,:])\n",
    "\n",
    "eeg_data = np.stack(eeg_list,axis=0)\n",
    "mel_data = np.stack(mel_list,axis=0)\n",
    "print(eeg_data.shape,mel_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 40\n"
     ]
    }
   ],
   "source": [
    "input_dim = eeg_data.shape[-1]\n",
    "output_dim = mel_data.shape[-1]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vqvae_encoder = VQVAEEncoder(mel_dim=output_dim,eeg_dim=input_dim,mel_output_dim=d_model,eeg_output_dim=d_model,n_embedding=n_embedding,embedding_dim=d_model).to(device)\n",
    "cpc = Cross_CPC(embedding_dim=d_model,hidden_dim=hidden_dim,context_dim=hidden_dim,num_layers=n_layer,predict_step=pred_size).to(device)\n",
    "mel_mi_net = CLUBSample_group(x_dim=d_model,y_dim=d_model,hidden_size=hidden_dim).to(device)\n",
    "eeg_mi_net = CLUBSample_group(x_dim=d_model,y_dim=d_model,hidden_size=hidden_dim).to(device)\n",
    "vqvae_decoder = VQVAEDecoder(mel_dim=output_dim,eeg_dim=input_dim,mel_output_dim=d_model,eeg_output_dim=d_model,embedding_dim=d_model).to(device)\n",
    "mel_vq_decoder = SemanticDecoder(input_dim=d_model,output_dim=output_dim).to(device)\n",
    "\n",
    "main_optimizer = torch.optim.Adam(chain(vqvae_encoder.parameters(),cpc.parameters(),vqvae_decoder.parameters()),lr=lr,betas=(b1,b2))\n",
    "mel_mi_net_optimizer = torch.optim.Adam(mel_mi_net.parameters(),lr=lr,betas=(b1,b2))\n",
    "eeg_mi_net_optimizer = torch.optim.Adam(eeg_mi_net.parameters(),lr=lr,betas=(b1,b2))\n",
    "mel_vq_decoder_optimizer = torch.optim.Adam(mel_vq_decoder.parameters(),lr=lr,betas=(b1,b2))\n",
    "# scheduler = MultiStepLR(main_optimizer,milestones=[10,20,30],gamma=0.5)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "loss_fn = lambda x,y:(criterion(x, y)+criterion(torch.exp(x),torch.exp(y))+criterion(dct(x,norm='ortho'),dct(y,norm='ortho')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_padding = np.zeros((1,eeg.shape[1]))\n",
    "eeg_list = []\n",
    "for idx in range(eeg.shape[0]):\n",
    "    if idx-prv_frame+1<0:\n",
    "        tmp = eeg[0:idx+1]\n",
    "        for _ in range(prv_frame-idx-1):\n",
    "            tmp=np.insert(tmp,0,data_padding,axis=0)\n",
    "        eeg_list.append(tmp)\n",
    "    else:\n",
    "        eeg_list.append(eeg[idx-prv_frame+1:idx+1])\n",
    "eeg = np.stack(eeg_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 3, 122)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=122, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerModel(\n",
       "    (encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "  (l3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=40, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pbar = tqdm.trange(epochs, desc=f\"Epochs\")\n",
    "model.load_state_dict(torch.load(f'./res/{pt}/{model_name}.pt')['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(torch.from_numpy(eeg).to(device).type(tensor_type)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(f'./logs/{pt}/{model_name}')\n",
    "origin_melspec_fig = plt.figure()\n",
    "librosa.display.specshow(melspec.T,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')        \n",
    "plt.title(f'{pt}-{word}-origin')\n",
    "writer.add_figure(tag=f\"{pt}-{word}-origin log Mel spectrogram\",figure=origin_melspec_fig)\n",
    "\n",
    "model_melspec_fig = plt.figure()\n",
    "librosa.display.specshow(model_output.T,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')        \n",
    "plt.title(f'{pt}-{word}-model')\n",
    "writer.add_figure(tag=f\"{pt}-{word}-model log Mel spectrogram\",figure=model_melspec_fig)\n",
    "plt.show()\n",
    "# librosa_melspec_fig = plt.figure()\n",
    "# # numWindows = int(np.floor((audio.shape[0]-window_length*audio_sameple_rate)/(frameshift*audio_sameple_rate)))\n",
    "# librosa_melspec = librosa.feature.melspectrogram(y=audio.astype(np.float32),sr=audio_sameple_rate,n_fft=400,hop_length=80,n_mels=80,center=False)\n",
    "# librosa_melspec = librosa.power_to_db(librosa_melspec, ref=np.max)\n",
    "# librosa.display.specshow(librosa_melspec,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')        \n",
    "# plt.title(f'{pt}-{word}-librosa')\n",
    "# plt.show()\n",
    "# # print(numWindows)\n",
    "# writer.add_figure(tag=f\"{pt}-{word}-librosa log Mel spectrogram\",figure=librosa_melspec_fig)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 40) (96, 13)\n",
      "(96, 40) (96, 13)\n",
      "19.382871546148376\n"
     ]
    }
   ],
   "source": [
    "model_mfcc = utils.toMFCC(model_output)\n",
    "mfcc = utils.toMFCC(melspec)\n",
    "eu_dis = 0\n",
    "for i in range(mfcc.shape[0]):\n",
    "    eu_dis += np.linalg.norm(model_mfcc[i] - mfcc[i])\n",
    "mcd = eu_dis/mfcc.shape[0]\n",
    "print(model_output.shape,model_mfcc.shape)\n",
    "print(melspec.shape,mfcc.shape)\n",
    "print(mcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('mel_files',f'{pt}_{test_word}_model.npy'),model_output.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
