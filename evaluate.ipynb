{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/data1/byzhao/EEG-AudioTransformer/utils.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "imp.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公共变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_path = r'./feat/words'\n",
    "pt = 'sub-06'\n",
    "test_word = 10\n",
    "config_path = r'./config'\n",
    "model_name = 'h4l6p3f40'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1290, 127) (20160,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(words_path,f'{pt}')\n",
    "filename = os.listdir(folder_path)[test_word]\n",
    "word_info = np.load(os.path.join(folder_path,filename),allow_pickle=True)\n",
    "word=word_info.item()['label']\n",
    "eeg=word_info.item()['eeg']\n",
    "audio=word_info.item()['audio']\n",
    "\n",
    "print(eeg.shape,audio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 0.025\n",
    "frameshift = 0.005\n",
    "eeg_sample_rate = 1024\n",
    "audio_sameple_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取高频eeg信号和音频信号的梅尔频谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 127) (247, 40)\n",
      "(246, 127) (246, 40)\n"
     ]
    }
   ],
   "source": [
    "eeg = utils.extractHG(eeg,eeg_sample_rate,windowLength=window_length,frameshift=frameshift)\n",
    "melspec = utils.extractMelSpecs(audio,audio_sameple_rate,windowLength=window_length,frameshift=frameshift)\n",
    "print(eeg.shape,melspec.shape)\n",
    "if melspec.shape[0]!=eeg.shape[0]:\n",
    "    minlen = min(melspec.shape[0],eeg.shape[0])\n",
    "    melspec = melspec[:minlen,:]\n",
    "    eeg = eeg[:minlen,:]\n",
    "print(eeg.shape,melspec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z均值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_mean = np.mean(eeg)\n",
    "eeg_std = np.std(eeg)\n",
    "eeg = (eeg-eeg_mean)/eeg_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import transformer\n",
    "imp.reload(transformer)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 40\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(config_path,f'{model_name}.json'),'r') as f:\n",
    "    cfg = json.load(f)['model_config']\n",
    "\n",
    "prv_frame = cfg['prv_frame']\n",
    "batch_size = cfg['batch_size']\n",
    "epochs = cfg['epochs']\n",
    "lr = cfg['lr']\n",
    "b1 = cfg['b1']\n",
    "b2 = cfg['b2']\n",
    "scaled_dim = cfg['scaled_dim']\n",
    "d_model = cfg['d_model']\n",
    "nhead = cfg['nhead']\n",
    "n_layer = cfg['n_layer']\n",
    "input_dim = eeg.shape[-1]\n",
    "output_dim = melspec.shape[-1]\n",
    "\n",
    "tensor_type = torch.cuda.FloatTensor\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# log_write = open(f\"./log/log_{pt}.txt\", \"w\") \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = transformer.Model(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    scaled_dim=scaled_dim,\n",
    "    prv_dim=prv_frame,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    n_layer=n_layer\n",
    ").to(device)\n",
    "\n",
    "# criterion = nn.MSELoss(reduction='mean').to(device)\n",
    "criterion = nn.L1Loss(reduction='mean').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=(b1,b2))\n",
    "\n",
    "print(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_padding = np.zeros((1,eeg.shape[1]))\n",
    "eeg_list = []\n",
    "for idx in range(eeg.shape[0]):\n",
    "    if idx-prv_frame+1<0:\n",
    "        tmp = eeg[0:idx+1]\n",
    "        for _ in range(prv_frame-idx-1):\n",
    "            tmp=np.insert(tmp,0,data_padding,axis=0)\n",
    "        eeg_list.append(tmp)\n",
    "    else:\n",
    "        eeg_list.append(eeg[idx-prv_frame+1:idx+1])\n",
    "eeg = np.stack(eeg_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 3, 127)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=127, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer): TransformerModel(\n",
       "    (encoder_layer): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "  (l3): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=40, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pbar = tqdm.trange(epochs, desc=f\"Epochs\")\n",
    "model.load_state_dict(torch.load(f'./res/{pt}/{model_name}.pt')['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(torch.from_numpy(eeg).to(device).type(tensor_type)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(f'./logs/{pt}/{model_name}')\n",
    "origin_melspec_fig = plt.figure()\n",
    "librosa.display.specshow(melspec.T,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')        \n",
    "plt.title(f'{pt}-{word}-origin')\n",
    "writer.add_figure(tag=f\"{pt}-{word}-origin log Mel spectrogram\",figure=origin_melspec_fig)\n",
    "\n",
    "model_melspec_fig = plt.figure()\n",
    "librosa.display.specshow(model_output.T,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')        \n",
    "plt.title(f'{pt}-{word}-model')\n",
    "writer.add_figure(tag=f\"{pt}-{word}-model log Mel spectrogram\",figure=model_melspec_fig)\n",
    "plt.show()\n",
    "# librosa_melspec_fig = plt.figure()\n",
    "# # numWindows = int(np.floor((audio.shape[0]-window_length*audio_sameple_rate)/(frameshift*audio_sameple_rate)))\n",
    "# librosa_melspec = librosa.feature.melspectrogram(y=audio.astype(np.float32),sr=audio_sameple_rate,n_fft=400,hop_length=80,n_mels=80,center=False)\n",
    "# librosa_melspec = librosa.power_to_db(librosa_melspec, ref=np.max)\n",
    "# librosa.display.specshow(librosa_melspec,sr=16000,hop_length=80,win_length=400,x_axis='time', y_axis='mel')\n",
    "# plt.colorbar(format='%+2.0f dB')        \n",
    "# plt.title(f'{pt}-{word}-librosa')\n",
    "# plt.show()\n",
    "# # print(numWindows)\n",
    "# writer.add_figure(tag=f\"{pt}-{word}-librosa log Mel spectrogram\",figure=librosa_melspec_fig)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 40) (246, 13)\n",
      "(246, 40) (246, 13)\n",
      "1.623258642696099\n"
     ]
    }
   ],
   "source": [
    "model_mfcc = utils.toMFCC(model_output)\n",
    "mfcc = utils.toMFCC(melspec)\n",
    "eu_dis = 0\n",
    "for i in range(mfcc.shape[0]):\n",
    "    eu_dis += np.linalg.norm(model_mfcc[i] - mfcc[i])\n",
    "mcd = eu_dis/mfcc.shape[0]\n",
    "print(model_output.shape,model_mfcc.shape)\n",
    "print(melspec.shape,mfcc.shape)\n",
    "print(mcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 40)\n",
      "(246, 40)\n"
     ]
    }
   ],
   "source": [
    "print(melspec.shape)\n",
    "print(model_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
